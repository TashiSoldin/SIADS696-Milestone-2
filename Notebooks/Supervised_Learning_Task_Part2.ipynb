{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8907c48e",
   "metadata": {},
   "source": [
    "# Supervised Learning Task Part 1 - Deep Learning Models\n",
    "SIADS 696 Milestone 2 - Team 24\n",
    "\n",
    "Notebook author: Seungdo Woo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a04100",
   "metadata": {},
   "source": [
    "This notebook addresses the ImportError caused by using the experimental `HalvingGridSearchCV` without enabling it. It imports `enable_halving_search_cv` from `sklearn.experimental` and uses 5-fold cross-validation for tuning and evaluation. It includes data upload, preprocessing with float32 conversion, baseline model evaluation, memory-efficient hyperparameter tuning with 20% sampling (with optional 40% comparison), and final model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572a9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload the dataset file from your local machine into this Colab environment.\n",
    "# # This will prompt you to select the local 'OCEAN_PROCESSED.CSV' file for upload.\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# # Rename the uploaded file to 'OCEAN_PROCESSED.CSV' if necessary.\n",
    "# import os\n",
    "# for fn in uploaded.keys():\n",
    "#     if fn != 'OCEAN_PROCESSED.CSV':\n",
    "#         os.rename(fn, 'OCEAN_PROCESSED.CSV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c8fc2",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099eadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Experimental import to enable HalvingGridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa: F401\n",
    "\n",
    "# Scikit-learn utilities for model training and evaluation\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, HalvingGridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5738c",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CAST  Latitude  Longitude  Year  Month  Day   Time Country  \\\n",
      "0  16493420    54.207     13.567  2000      1    3  10.72      DE   \n",
      "1  16493421    54.007     14.233  2000      1    3  13.68      DE   \n",
      "2  16493422    53.938     14.225  2000      1    3  14.63      DE   \n",
      "3  16493423    54.113     14.117  2000      1    3  16.02      DE   \n",
      "4  13746677    35.795    129.532  2000      1    4   3.92      KR   \n",
      "\n",
      "   Bottom depth  Depth  Temperature  Salinity  Oxygen  Phosphate  Silicate  \\\n",
      "0           9.0    1.5         2.50      7.40   403.0       0.53     27.30   \n",
      "1          11.0    1.5         2.10      7.10   406.0       1.15     30.30   \n",
      "2           7.0    1.5         2.30      6.40   381.0       1.43     32.90   \n",
      "3          14.0    1.5         2.40      8.50   427.0       0.70     20.90   \n",
      "4          46.0    0.0        14.62     34.19   245.0       0.43      8.13   \n",
      "\n",
      "   Nitrate  Pressure  \n",
      "0     9.70   1.50000  \n",
      "1    18.20   1.50000  \n",
      "2    20.20   1.50000  \n",
      "3    10.00   1.50000  \n",
      "4     7.38  -2.77566  \n"
     ]
    }
   ],
   "source": [
    "# file_path_formatted = '/content/drive/MyDrive/Colab Notebooks/MADS/SIADS 696/Data/ocean_processed.csv' # Google Colab Path to Processed Ocean Dataset\n",
    "file_path_formatted = '../Data/ocean_processed.csv' # Local Path to Processed Ocean Dataset\n",
    "df = pd.read_csv(file_path_formatted)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49ec9b",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b485418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 482320 Test samples: 120580\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['CAST', 'Oxygen'])\n",
    "y = df['Oxygen']\n",
    "\n",
    "# Label encode the 'Country' column if necessary\n",
    "if X['Country'].dtype == object:\n",
    "    le = LabelEncoder()\n",
    "    X['Country'] = le.fit_transform(X['Country'])\n",
    "\n",
    "# Split into training and test sets (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32 to reduce memory usage\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "print('Training samples:', X_train.shape[0], 'Test samples:', X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0c1037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CV RMSE: 47.65 ± 21.19\n",
      "Baseline CV R²:  0.7480 ± 0.2453\n"
     ]
    }
   ],
   "source": [
    "# Baseline neural network model inside a pipeline\n",
    "# Increase max_iter and enable early_stopping to reduce convergence warnings.\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42,\n",
    "                         early_stopping=True, n_iter_no_change=10, tol=1e-4))\n",
    "])\n",
    "\n",
    "# Configure 5-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the baseline model using cross-validation\n",
    "cv_results = cross_validate(\n",
    "    baseline_pipeline, X_train, y_train,\n",
    "    cv=cv,\n",
    "    scoring={'rmse': 'neg_root_mean_squared_error', 'r2': 'r2'},\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Convert negative RMSE scores back to positive values\n",
    "rmse_scores = -cv_results['test_rmse']\n",
    "r2_scores = cv_results['test_r2']\n",
    "\n",
    "print(f'Baseline CV RMSE: {rmse_scores.mean():.2f} ± {rmse_scores.std():.2f}')\n",
    "print(f'Baseline CV R²:  {r2_scores.mean():.4f} ± {r2_scores.std():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2effb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 24116\n",
      "max_resources_: 96464\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 24116\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END ...mlp__alpha=0.0001, mlp__hidden_layer_sizes=(50,); total time=   4.0s\n",
      "[CV] END ...mlp__alpha=0.0001, mlp__hidden_layer_sizes=(50,); total time=   5.5s\n",
      "[CV] END ...mlp__alpha=0.0001, mlp__hidden_layer_sizes=(50,); total time=   4.9s\n",
      "[CV] END ...mlp__alpha=0.0001, mlp__hidden_layer_sizes=(50,); total time=   4.2s\n",
      "[CV] END ...mlp__alpha=0.0001, mlp__hidden_layer_sizes=(50,); total time=   4.9s\n",
      "[CV] END ..mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100,); total time=  10.0s\n",
      "[CV] END ..mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100,); total time=   6.0s\n",
      "[CV] END ..mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100,); total time=   8.5s\n",
      "[CV] END ..mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100,); total time=   7.8s\n",
      "[CV] END ..mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100,); total time=   8.4s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  14.1s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  14.5s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  11.8s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  15.4s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=   9.8s\n",
      "[CV] END ....mlp__alpha=0.001, mlp__hidden_layer_sizes=(50,); total time=   4.8s\n",
      "[CV] END ....mlp__alpha=0.001, mlp__hidden_layer_sizes=(50,); total time=   5.1s\n",
      "[CV] END ....mlp__alpha=0.001, mlp__hidden_layer_sizes=(50,); total time=   5.2s\n",
      "[CV] END ....mlp__alpha=0.001, mlp__hidden_layer_sizes=(50,); total time=   4.4s\n",
      "[CV] END ....mlp__alpha=0.001, mlp__hidden_layer_sizes=(50,); total time=   4.9s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=  10.5s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=   6.1s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=   8.4s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=   7.8s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=   8.8s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  13.0s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  12.4s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=   9.2s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  13.9s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=   8.6s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 48232\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=  16.0s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=  14.9s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=   0.7s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=  14.9s\n",
      "[CV] END ...mlp__alpha=0.001, mlp__hidden_layer_sizes=(100,); total time=  14.0s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  23.9s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  29.2s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=515.3min\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  25.0s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=  31.9s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  28.1s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  17.7s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  18.7s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  20.8s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  21.2s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 96464\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time= 2.6min\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=11.1min\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time= 1.8min\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time=   4.0s\n",
      "[CV] END mlp__alpha=0.001, mlp__hidden_layer_sizes=(100, 100); total time= 1.1min\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  54.7s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  50.2s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=  55.1s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time=   4.0s\n",
      "[CV] END mlp__alpha=0.0001, mlp__hidden_layer_sizes=(100, 100); total time= 1.8min\n",
      "Best parameters: {'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100, 100)}\n",
      "Best CV RMSE (mean): 24.44\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using HalvingGridSearchCV with 5-fold CV\n",
    "\n",
    "# Create a subsample of the training data for tuning (20%)\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "    'mlp__alpha': [0.0001, 0.001]\n",
    "}\n",
    "\n",
    "# Create the tuning pipeline\n",
    "tune_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(max_iter=500, random_state=42, early_stopping=True,\n",
    "                         n_iter_no_change=10, tol=1e-4))\n",
    "])\n",
    "\n",
    "# Use 5-fold CV for tuning\n",
    "cv_tune = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform halving grid search\n",
    "grid_search = HalvingGridSearchCV(\n",
    "    estimator=tune_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    factor=2,\n",
    "    cv=cv_tune,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search on the subsampled data\n",
    "grid_search.fit(X_tune, y_tune)\n",
    "\n",
    "# Output best parameters and CV RMSE\n",
    "best_params = grid_search.best_params_\n",
    "best_cv_rmse = -grid_search.best_score_\n",
    "print('Best parameters:', best_params)\n",
    "print(f'Best CV RMSE (mean): {best_cv_rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a53d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 24116\n",
      "max_resources_: 96464\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 24116\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 48232\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 96464\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Sample 20% -> Best params: {'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (50,)}, CV RMSE: 26.12\n",
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 48232\n",
      "max_resources_: 192928\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 48232\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 96464\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 192928\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Sample 40% -> Best params: {'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': (100, 100)}, CV RMSE: 27.47\n"
     ]
    }
   ],
   "source": [
    "# Optional: Compare 20% vs 40% sampling for hyperparameter tuning\n",
    "def tune_with_sample(sample_fraction):\n",
    "    X_sample, _, y_sample, _ = train_test_split(X_train, y_train, train_size=sample_fraction, random_state=42)\n",
    "    param_grid = {\n",
    "        'mlp__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "        'mlp__alpha': [0.0001, 0.001]\n",
    "    }\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPRegressor(max_iter=500, random_state=42, early_stopping=True,\n",
    "                             n_iter_no_change=10, tol=1e-4))\n",
    "    ])\n",
    "    cv_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    halving_search = HalvingGridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        factor=2,\n",
    "        cv=cv_local,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    halving_search.fit(X_sample, y_sample)\n",
    "    return halving_search.best_params_, -halving_search.best_score_\n",
    "\n",
    "# Run comparison for 20% and 40% samples\n",
    "for frac in [0.2, 0.4]:\n",
    "    params, rmse = tune_with_sample(frac)\n",
    "    print(f'Sample {int(frac*100)}% -> Best params: {params}, CV RMSE: {rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb1c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 601.61 s\n",
      "Test RMSE: 33.08\n",
      "Test R²:  0.9006\n"
     ]
    }
   ],
   "source": [
    "# Train the best model on the full training set and evaluate on the test set\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the model on full training data\n",
    "start_time = time.time()\n",
    "best_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Test metrics\n",
    "# Compute RMSE manually because older versions of scikit-learn may not support 'squared' parameter\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(mse)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Training time: {train_time:.2f} s')\n",
    "print(f'Test RMSE: {test_rmse:.2f}')\n",
    "print(f'Test R²:  {test_r2:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
